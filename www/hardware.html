<!--
   EMULAB-COPYRIGHT
   Copyright (c) 2000-2003, 2005 University of Utah and the Flux Group.
   All rights reserved.
  -->
<center>
<h1>
    Hardware Overview, "Emulab Classic"
</h1>
</center>

<h3>Test Nodes</h3>
<ul>
<a name="tbpcs"></a>

<ul>
<li>160 3.0 GHz 64-bit Xeon processors (currently being racked up).
<ul>
<li> 3.0 GHz 64-bit Xeon, 800Mhz FSB
<li> Based on the
     <a href =
     "http://www.dell.com/downloads/global/products/pedge/en/2850_specs.pdf">
      Dell Poweredge 2850</a>
<li> 2GB 400Mhz DDR2 RAM
<li> Multiple PCI-X 64/133 and 64/100 busses
<li> 6 10/100/1000 Intel NICs spread across the busses
	(one NIC is the control net)
<li> 2 x 146GB 10,000 RPM SCSI disks
</ul>
<p>

<a name="tbpc850"></a>
<li>128 <a href=shownodetype.php3?node_type=pc850>pc850</a>
             PC nodes (<b>pc41-pc168</b>), consisting of:
<ul>
<li> 850MHz Intel Pentium III processors.
<li> Based on the
<a
href="http://www.intel.com/support/motherboards/server/isp1100/">
Intel ISP1100 </a> 1U server platform (old reliable BX chipset).
<li> 512MB PC133 ECC SDRAM.
<li> 5
<a
href="http://support.intel.com/support/network/adapter/pro100/index.htm">
Intel EtherExpress Pro</a> 10/100Mbps Ethernet ports:
<ul>
	<li>2 builtin on the motherboard
		(<code>eth2/eth3</code> in Linux, <code>fxp0/fxp1</code> in FreeBSD)</i>
	<li>2 on an
	    <a href = "http://www.intel.com/network/connectivity/resources/doc_library/tech_specs/pro100dport.htm">
	    Intel EtherExpress Pro 100+ Dual-Port Server Adapter</a>
		(<code>eth0/eth1</code> in Linux, <code>fxp2/fxp3</code> in FreeBSD)
	<li>1 on a single-port Intel EtherExpress Pro/100B Adapter
		(<code>eth4</code> in Linux, <code>fxp4</code> in FreeBSD)
</ul>
<li> 
40GB IBM 60GXP 7200RPM ATA/100 IDE hard drive.
<li> Floppy drive.
</ul>
<p>

<a name="tbpc600"></a>
<li>40 <a href=shownodetype.php3?node_type=pc600>pc600</a>
        PC nodes (<b>pc1-40</b>), consisting of:

<ul>
<li> 600MHz Intel Pentium III "Coppermine" processors.
<li> 
Asus P3B-F (6 PCI/1 ISA slot) motherboard (old reliable BX chipset).
<li> 256MB PC100 ECC SDRAM.
<li> 5
<a
href="http://support.intel.com/support/network/adapter/pro100/index.htm">
Intel EtherExpress Pro/100B</a> 10/100Mbps Ethernet cards.
<li>
13GB IBM 34GXP DPTA-371360 7200RPM IDE hard drive.
<li> Floppy drive
<li> Cheap video card (Jaton Riva 128ZX AGP w/4MB video RAM)
<li> All in a nice but overweight rackmount case on rails:
Antec IPC3480B, with 300W PS and extra fan.
</ul>
<p>

<a name="tbpc3000w"></a>
<li>18 <a href=shownodetype.php3?node_type=pc3000w>pc3000w</a>
        wireless PC nodes (<b>pcwf1-18</b>), consisting of:
<ul>
<li> 3.0GHz Intel Pentium 4 processors.
<li> 1GB DDR-400MHz (PC3200) SDRAM.
<li> 2 <a href="http://netgear.com/products/details/WAG311.php">Netgear WAG311
802.11a/b/g</a> (Atheros) wifi cards.
<li> 2 Ethernet ports:
<ul>
	<li>1 <a href="http://support.intel.com/support/network/adapter/index.htm">Intel
	Pro 1000</a> 10/100/1000Mbps builtin on the motherboard.
	<li>1 <a
	href="http://support.intel.com/support/network/adapter/pro100/index.htm">Intel
	EtherExpress Pro/100 S</a> 10/100Mbps Ethernet card.
</ul>
<li> 2 Western Digital WDXL80 120GB 7200RPM Serial ATA hard drives.
<li> Floppy and CD-ROM drive.
</ul>


</ul>
<h3>Servers</h3>
<ul>

<li> a users, file, and serial line server (<b>users.emulab.net</b>), consisting of:

<ul>
<li> Dual 500MHz Intel Pentium III processors
<li> 
Intel L440GX+ motherboard (the GX+ chipset)
<li> 512MB PC100 ECC SDRAM
<li> 90 GB disk space: 5 
Quantum Atlas IV 18GB 7200RPM Wide LVD SCSI hard drives
<li> 3 64-port
<a href="http://www.cyclades.com/products/svrbas/zseries.htm">
Cyclades-Ze PCI Multiport Serial Boards</a> (model number SEZ0050).
</ul>
<p>

<li> a DB, web, DNS and operations server, consisting of:

<ul>
<li>
Dell PowerEdge 2550 Rack Mount Server
<li> Single 1000MHz Intel Pentium III processor
<li> 512MB PC133 ECC SDRAM
<li> Dual-Channel On-board RAID (5) Controller 128MB Cache (2-Int Channels)
<li> Five 18GB Ultra3 (Ultra160) SCSI 10K RPM Hot Plug Hard Drives
<li> Dual Redundant 330 Watt Power Supplies
<li> Integrated Broadcom Gigabit BaseT and Intel Pro/100+ NICs
</ul>
<p>

<li> a second serial line server, consisting of:
<ul>
<li> 1U box from <a href = "http://www.asacomputers.com/">ASA Computers</a>
<li> 1 64-port
	<a href="http://www.cyclades.com/products/svrbas/zseries.htm">
	Cyclades-Ze PCI Multiport Serial Board</a> (model number SEZ0050).
</ul>
<p>

<li> a serial line server for critical emulab machines, consisting of:
<ul>
<li> An ISP1100 box, like the "pc850" nodes above.
<li> a 4-port serial card.
</ul>
<p>

</ul>
<h3>Switches and Routers</h3>
<ul>

<li> 4 <a href = "http://www.cisco.com/warp/public/cc/pd/si/casi/ca6000/prodlit/c6000_ds.htm">
Cisco 6509 high-end switches</a>.
	Three function as the
	<a name="tbbackplane"></a>
	<em>testbed backplane</em> ("programmable patch panel"),
	each filled with a
	<a href = "http://www.cisco.com/warp/public/cc/pd/si/casi/ca6000/prodlit/6nam_ds.htm">
	Network Analysis Module</a>
	and seven 48-port 10/100 ethernet modules,
	giving 336 100Mbps ethernet ports on each.  They are linked with 2 Gbit interfaces.
	The final 6509 contains an MSFC router card and functions as the
	<a name="tbcorerouter"></a>
	<em>core router</em> for the testbed,
	providing "control" interfaces for the test nodes as well as
	regulating access to the testbed servers and the outside world.
	This switch is configured with full router software,
	Gigabit ethernet, OC-12 ATM (~600Mbps), and more 
	10/100 Ethernet ports.
        <!-- Another, but no pix: http://www.cisco.com/univercd/cc/td/doc/pcat/ca6000.htm -->
<p>

</ul>
<h3>Power Controllers</h3>
<ul>

<li>10 APC MasterSwitch AP9210 8 port power controllers.<br>
    (The AP9210 is discontinued; replaced in the product line by the
    AP9211.)
<p>

<li>7 <a href="http://www.baytechdcd.com/products/rpc27.shtml">
	BayTech RPC27</a> 20 port remote power controllers.<br>
<p>

</ul>
<h3>Racks</h3>
<ul>

<li>13
	Wrightline "Tech I" racks: 44U, 34" deep, 2 are
	24" wide with cable management; the rest are 19" wide.
	(Aug 2001: these appear to have been discontinued or renamed.)
<p>

</ul>

<p><h2>Layout</h2><p>

Four ethernet ports on each PC node are connected to the
      <a href="#tbbackplane">testbed backplane</a>.
      All 672 ports can be connected in arbitrary ways by setting up VLANs
      on the switches via remote configuration tools. 
      Cisco 6500 Switch backplane bandwidth is supposed to be near 50Gb/s,
      <!-- but we've heard rumors that it's worse. -->
      though between the testbed backplane switches,
      bandwidth is currently limited to 2Gb/s.
<p>

The fifth ethernet port on each PC is connected to the
      <a href="#tbcorerouter">core router</a>.
      Thus each PC has a full duplex 100Mbps connection to the servers.
      These connections are for dumping 
      data off of the nodes and such, without interfering with
      the experimental interfaces. The only impact on the node is
      processor and disk use, and bandwidth on the PCI bus.
<p>


