<center>
<h1>
    Hardware Overview
</h1>
</center>

<h3>Test Nodes</h3>
<ul>
<a name="tbpc850"></a>
<li>128 "pc850" PC nodes (<b>pc41-pc168</b>), consisting of:

<ul>
<li> 850MHz Intel Pentium III processors.
<li> Based on the
<a
href="http://www.intel.com/network/products/isp1100.htm">
Intel ISP1100 </a> 1U server platform (old reliable BX chipset).
<li> 512MB PC133 ECC SDRAM.
<li> 5
<a
href="http://support.intel.com/support/network/adapter/pro100/index.htm">
Intel EtherExpress Pro</a> 10/100Mbps Ethernet ports:
<ul>
	<li>2 builtin on the motherboard
		(<code>eth2/eth3</code> in Linux, <code>fxp0/fxp1</code> in FreeBSD)</i>
	<li>2 on an
	    <a href = "http://www.intel.com/network/connectivity/resources/doc_library/tech_specs/pro100dport.htm">
	    Intel EtherExpress Pro 100+ Dual-Port Server Adapter</a>
		(<code>eth0/eth1</code> in Linux, <code>fxp2/fxp3</code> in FreeBSD)
	<li>1 on a single-port Intel EtherExpress Pro/100B Adapter
		(<code>eth4</code> in Linux, <code>fxp4</code> in FreeBSD)
</ul>
<li> <a href="http://www.storage.ibm.com/hdd/desk/ds60gxp.htm">
40GB IBM 60GXP 7200RPM ATA/100 IDE</a> hard drive.
<li> Floppy drive.
</ul>
<p>

<a name="tbpc600"></a>
<li>40 "pc600" PC nodes (<b>pc1-40</b>), consisting of:

<ul>
<li> 600MHz Intel Pentium III "Coppermine" processors.
<li> 
<a
href="http://www.asus.com/Products/Motherboard/Pentiumpro/P3b-f/index.html">
Asus P3B-F (6 PCI/1 ISA slot)</a> motherboard (old reliable BX chipset).
<li> 256MB PC100 ECC SDRAM.
<li> 5
<a
href="http://support.intel.com/support/network/adapter/pro100/index.htm">
Intel EtherExpress Pro/100B</a> 10/100Mbps Ethernet cards.
<li> <a href="http://www.storage.ibm.com/hardsoft/diskdrdl/desk/ds34gxp.htm">
13GB IBM 34GXP DPTA-371360 7200RPM IDE</a> hard drive.
<li> Floppy drive
<li> Cheap video card (Jaton Riva 128ZX AGP w/4MB video RAM)
<li> All in a nice but overweight rackmount case on rails:
<a href = "http://www.antec-inc.com/product/cases/en_rack.html">
Antec IPC3480B</a>, with 300W PS and extra fan.
</ul>
<p>

<a name="tbshark"></a>
<li>160 diskless <a href = "http://www.research.digital.com/SRC/iag/">
	Compaq DNARD "Sharks"</a> edge nodes (<b>sh[1-20]-[1-8]</b>),
	consisting of:

<ul>
<li> 233 Mhz StrongARM processors.
<li> 32MB RAM.
<li> 1 10Mbps ethernet interface.
</ul>
<p>

</ul>
<h3>Servers</h3>
<ul>

<li> a users, file, and serial line server (<b>users.emulab.net</b>), consisting of:

<ul>
<li> Dual 500MHz Intel Pentium III processors
<li> <a href="http://developer.intel.com/design/servers/l440gx/index.htm">
Intel L440GX+</a> motherboard (the GX+ chipset)
<li> 512MB PC100 ECC SDRAM
<li> 90 GB disk space: 5 
<a href="http://www.quantum.com/products/hdd/atlas_iv/atlas_iv_overview.htm">
Quantum Atlas IV 18GB 7200RPM Wide LVD SCSI</a> hard drives
<li> 3 64-port
<a href="http://www.cyclades.com/products/svrbas/zseries.htm">
Cyclades-Ze PCI Multiport Serial Boards</a> (model number SEZ0050).
</ul>
<p>

<li> a DB, web, DNS and operations server, consisting of:

<ul>
<li> <a
href="http://www.dell.com/us/en/hied/products/model_pedge_pedge_2550.htm">
Dell PowerEdge 2550</a> Rack Mount Server
<li> Single 1000MHz Intel Pentium III processor
<li> 512MB PC133 ECC SDRAM
<li> Dual-Channel On-board RAID (5) Controller 128MB Cache (2-Int Channels)
<li> Five 18GB Ultra3 (Ultra160) SCSI 10K RPM Hot Plug Hard Drives
<li> Dual Redundant 330 Watt Power Supplies
<li> Integrated Broadcom Gigabit BaseT and Intel Pro/100+ NICs
</ul>
<p>

<li> a second serial line server, consisting of:
<ul>
<li> 1U box from <a href = "http://www.asacomputers.com/">ASA Computers</a>
<li> 1 64-port
	<a href="http://www.cyclades.com/products/svrbas/zseries.htm">
	Cyclades-Ze PCI Multiport Serial Board</a> (model number SEZ0050).
</ul>
<p>

<li> a serial line server for critical emulab machines, consisting of:
<ul>
<li> An ISP1100 box, like the "pc850" nodes above.
<li> a 4-port serial card.
</ul>
<p>

</ul>
<h3>Switches and Routers</h3>
<ul>

<li> 4 <a href = "http://www.cisco.com/warp/public/cc/pd/si/casi/ca6000/prodlit/c6000_ds.htm">
Cisco 6509 high-end switches</a>.
	Three function as the
	<a name="tbbackplane"></a>
	<em>testbed backplane</em> ("programmable patch panel"),
	each filled with a
	<a href = "http://www.cisco.com/warp/public/cc/pd/si/casi/ca6000/prodlit/6nam_ds.htm">
	Network Analysis Module</a>
	and seven 48-port 10/100 ethernet modules,
	giving 336 100Mbps ethernet ports on each.  They are linked with 2 Gbit interfaces.
	The final 6509 contains an MSFC router card and functions as the
	<a name="tbcorerouter"></a>
	<em>core router</em> for the testbed,
	providing "control" interfaces for the test nodes as well as
	regulating access to the testbed servers and the outside world.
	This switch is configured with full router software,
	Gigabit ethernet, OC-12 ATM (~600Mbps), and more 
	10/100 Ethernet ports.
        <!-- Another, but no pix: http://www.cisco.com/univercd/cc/td/doc/pcat/ca6000.htm -->
<p>

</ul>
<h3>Power Controllers</h3>
<ul>

<li>10 APC MasterSwitch AP9210 8 port power controllers.<br>
    (The AP9210 is discontinued; replaced in the product line by the
    <a href="http://www.apc.com/products/masterswitch/index.cfm">AP9211</a>.)
<p>

<li>7 <a href="http://www.baytechdcd.com/products/rpc27.shtml">
	BayTech RPC27</a> 20 port remote power controllers.<br>
<p>

</ul>
<h3>Racks</h3>
<ul>

<li>13 <a href = "http://www.wrightline.com/products/freestnd.htm">
	Wrightline "Tech I" racks</a>: 44U, 34" deep, 2 are
	24" wide with cable management; the rest are 19" wide.
	(Aug 2001: these appear to have been discontinued or renamed.)
<p>

</ul>

<p><h2>Layout</h2><p>

Four ethernet ports on each PC node are connected to the
      <a href="#tbbackplane">testbed backplane</a>.
      All 672 ports can be connected in arbitrary ways by setting up VLANs
      on the switches via remote configuration tools. 
      Cisco 6500 Switch backplane bandwidth is supposed to be near 50Gb/s,
      <!-- but we've heard rumors that it's worse. -->
      though between the testbed backplane switches,
      bandwidth is currently limited to 2Gb/s.
<p>

The fifth ethernet port on each PC is connected to the
      <a href="#tbcorerouter">core router</a>.
      Thus each PC has a full duplex 100Mbps connection to the servers.
      These connections are for dumping 
      data off of the nodes and such, without interfering with
      the experimental interfaces. The only impact on the node is
      processor and disk use, and bandwidth on the PCI bus.
<p>

The DNARD Sharks are arranged by "shelves."  A shelf holds 8 sharks
      each of which is connected by a 10Mbps link to an
      8+2 10/100 ethernet switch from Asante.  The Asante switches
      are connected via a 100Mbps link to the testbed backplane.
      Thus each shelf of 8 sharks
      is capable of generating up to 80Mbps to the backplane.


