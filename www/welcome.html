<html>
<head>
	<title>Utah Network Testbed - Operations Home</title>
	<link rel='stylesheet' href='tbstyle.css' type='text/css'>
	<base href='https://www.emulab.net/' target='dynamic'>
</head>
<body>

<h1>Testbed/Emulab Operations</h1>

This is the Web-based operational interface to the testbed.
For details on the testbed itself and why it's special, see the
<a href = "http://www.cs.utah.edu/flux/testbed/">base</a> and
<a href = "http://www.cs.utah.edu/flux/testbed/doc/">documentation</a> pages.
<p>

To protect data you submit, SSL is used to encrypt data as it
is transferred accross the Internet. Therefore, you will need to
access these pages with a browser that supports SSL.  We recommend
Netscape 4.0 or later, or presumably a recent IE, and also recommend a
screen resolution of at least 800x600 to avoid excessive scrolling.
<p>

To enhance operation, we make use of a "Cookie" that holds your login
name. Therefor, you will need to enable cookies on your browser. 

<h3>What is it?</h3>

<a href = "http://www.cs.utah.edu/flux/testbed/doc/index.html#HW">Raw hardware</a>--
lots of it-- with configuration and management tools.
<!-- lots of them.  [not yet so many] -->
During an experiment's time slots, the experiment (and associated researchers)
get exclusive use of the
assigned machines, including root access if desired.  Until we finish designing
and building smarter scheduling and state-saving software, and obtain the
disk space, scheduling is manual and done at coarse granularity
(days).

<p> We provide some default software (e.g. Linux and
FreeBSD on the PCs, NetBSD on the Sharks) that many users may want.
But fundamentally, the software you run on it, including all bits on
the disks, is replaceable and up to you.  The same applies to the network's
characteristics, including its topology: configurable by users.

<h3>A few hardware and usage details, please</h3>

Right now: a FreeBSD operations node with 90 GB of disk space,
on which all users get full Unix accounts, ssh-accessible from the world.
40 PCs each with 5 100 Mbit cards and a 13 GB disk, plus 160
StrongARM-based "Sharks," all connected by 2 miles of cable thru high
end Cisco switches.  Full details available on the
<a href = "http://www.cs.utah.edu/flux/testbed/doc/index.html#HW">hardware page</a>.
<p>

When an experiment's machines are active, appropriate Unix accounts
are also provided on those nodes.

<h3><a href = "policies.html">Administrative Policies and Disclaimer</a></h3>

Most any legitimate research/experimental use is allowed, including
use by companies.  Of course, when demand exceeds supply we will have
to assign priorities to projects, but the hardware base will be expanding.


<h3><a href = "auth.html">Authorization Scheme, Policy, and "How To Get Started"</a></h3>

It's hierarchical: we authorize a project under a principal
investigator (e.g. a faculty member) and delegate authority
to that person to authorize the project's members--  and
accountability for their behavior. 


<h3>Notes on security</a></h3>
<!-- <a href = "security.html"> -->

Each project gets a unique Unix group and its own protected subtree on
the ops node.  That subtree is NFS-exported only to that group's
assigned test machines.  We don't currently protect against spoofing
on the control network, so there are vulnerabilities.  The test networks
are fully separated between experiments by way of VLANs.


<h3>How to contact Testbed Ops</h3>

For questions or problems, send email to
<a href="mailto:testbed-ops@flux.cs.utah.edu">
Testbed Ops (testbed-ops@flux.cs.utah.edu)</a>.    
<p>


<hr>
<address>testbed-www@flux.cs.utah.edu<br>
Last modified on Oct 31, 2000</address>
</body>
</html>
