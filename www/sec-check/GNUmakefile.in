#
# EMULAB-COPYRIGHT
# Copyright (c) 2000-2006 University of Utah and the Flux Group.
# All rights reserved.
#

# See README-howto.txt for usage details.
#
# Notice that the output lists are put into the source dir.
# Spidered HTML pages go in (subdirs of) the obj dir.
#
# Top-level targets:
all_tasks = src_forms activate spider forms_coverage input_coverage normal probe
# Stats from the top-level tasks:
msgs_tasks = src_msg site_msg forms_msg input_msg
.PHONY: all $(all_tasks) msgs $(msgs_tasks)
all: $(all_tasks)
msgs: $(msgs_tasks)

SRCDIR		= @srcdir@
RESDIR		= $(SRCDIR)/results

TESTBED_SRCDIR	= @top_srcdir@

OBJDIR		= ../..
SUBDIR		= www/sec-check
SRCWWW		= $(TESTBED_SRCDIR)/www

OURDOMAIN       = @OURDOMAIN@

# Delete a target file if the command fails in the middle of changing it.
.DELETE_ON_ERROR:

#================================================================
# src_forms: Grep the sources for <form and make up a list of php form files.
src_forms: src_list src_msg

# All of the forms lines.
SRC_FORMS	= $(RESDIR)/src_forms.list
# Just the files list.
SRC_FILES	= $(RESDIR)/src_files.list
src_list: $(SRC_FORMS) $(SRC_FILES)
$(SRC_FORMS):
	# Ignore any Emacs backup files with tilde's in the filenames.
	(cd $(SRCWWW); \
	  find . -maxdepth 1 -name '*.php*' -print0 | \
	    xargs -0 grep -n '<form' | fgrep -v /save/ | \
	    sed '/^[^:]*~/d' ) | sort > $(SRC_FORMS)
$(SRC_FILES): $(SRC_FORMS)
	# Just filenames; get rid of the "./" prefix and ":<lnum>: <form" suffix.
	sed -e 's|^[^:]*/||' -e 's|:.*||' $(SRC_FORMS) | uniq > $(SRC_FILES)

src_msg: src_list
	@echo "** `wc -l < $(SRC_FORMS)` separate forms" \
	     "are on `wc -l < $(SRC_FILES)` code pages.  **" | tr -s " "

#================================================================
# Lots of common stuff for wget.
.PHONY: login admin logout

# Need an Emulab-in-Emulab experiment running, e.g. vulnElab.ns .
EinE_proj	= testbed
EinE_exp	= vulnElab
EinE_boss	= myboss
EinE_ops	= myops

# Login info for the inner Emulab.
uid		= $(USER)
### It's better to log in a browser and change your password in Edit Profile
### in the inner Elab to this string, than to put your real password here.
pswd		= EinE_tmp
# Real email address for confirmation messages.
email_dest	= flux.utah.edu
real_email	= $(uid)@$(email_dest)

dom		= $(EinE_proj).$(OURDOMAIN)
boss		= $(EinE_boss).$(EinE_exp).$(dom)
ops		= $(EinE_ops).$(EinE_exp).$(dom)
root		= http://$(boss)
sroot		= https://$(boss)

COOKIES		= cookies.txt
sv_cookies	= --save-cookies $(COOKIES)
ld_cookies	= --load-cookies $(COOKIES)
ld_cookies_subdir = --load-cookies ../$(COOKIES)
cookie_args	= --keep-session-cookies --no-check-certificate
wget_args	= -S -k $(cookie_args) $(ld_cookies)
wget_args_subdir= -S -k $(cookie_args) $(ld_cookies_subdir)

# Check results.
success_cmd = fgrep -f $(SRCDIR)/success.txt
failure_cmd = fgrep -f $(SRCDIR)/failure.txt

# Log in and create a current cookies.txt file.
# Args are uid and password.  The password must match your login (see above.)
logindir = logins
login_user = @ if [ ! -d $(logindir) ]; then mkdir $(logindir); fi; \
	wget -S -dv $(cookie_args) $(sv_cookies) \
	-o $(logindir)/login.log -O $(logindir)/login_$(1).html \
	--post-data "uid=$(1)&password=$(2)&login=Login" \
	$(sroot)/login.php3; \
	if $(failure_cmd) $(logindir)/login_$(1).html; then \
	      (echo "*** LOGIN FAILURE"; exit 1); \
	fi
login_sys := $(call login_user,$(uid),$(pswd))
login: logout
	$(login_sys)

# Log in above, then use this to toggle the admin bit on.
admin_cmd = @ if [ ! -d $(logindir) ]; then mkdir $(logindir); fi; \
	wget -S -dv $(cookie_args) $(ld_cookies) \
	-o $(logindir)/admin.log -O $(logindir)/admin.html \
	"$(sroot)/toggle.php?target_uid=$(uid)&type=adminon&value=1"; \
	if $(failure_cmd) $(logindir)/admin.html; then \
	      (echo "*** ADMIN FAILURE"; exit 1); \
	fi 
admin: login
	$(admin_cmd)

# Must be logged out to see the public view.
logout_cmd = @ if [ ! -d $(logindir) ]; then mkdir $(logindir); fi; \
	wget -S -dv $(cookie_args) $(ld_cookies)\
	-o $(logindir)/logout.log -O $(logindir)/logout.html \
	"$(sroot)/logout.php3?target_uid=$(uid)"
logout:
	$(logout_cmd)

#================================================================
# Factored-out commands for the "call" function.

# Format html args separated by a single space into an argument string with &'s.
# Putting them on separate lines with a backslash also results in a single space.
#
# The $(empty) var trick allows a subst arg of " " (a single space.)
# Found in the GNU make info node for Syntax of Functions.
empty :=
# Encode backslashed spaces in strings as %20, and double-quotes as %22.
fmt_html_args = $(subst $(empty) ,&,$(subst \ ,%20,$(subst \",%22,$(strip $(1)))))

# Post a form via wget and check the returned page.
# Args are [root],php_infile,[html_outfile],html_args .
# root defaults to $(sroot) .  html_outfile defaults to php_infile.html .
outdir = activate.wget
define wget_post
	@ if [ ! -d $(outdir) ]; then mkdir $(outdir); fi
	@ root_dir=$(strip $(1)) \
	  php_infile=$(strip $(2)) \
	  html_outfile=$(strip $(3)); \
	  post_args="$(call fmt_html_args, $(4))"; \
	  echo "==== $(outdir) $${root_dir} $${php_infile} $${html_outfile}.html ===="; \
	  echo "cd $(outdir); wget $(wget_args_subdir) \
	    -O $${html_outfile:=$(2).html} \
	    -o $${html_outfile}.log \
	    --post-data '$${post_args}' \
	    $${root_dir:-$(sroot)}/$${php_infile}" \
	    > $(outdir)/$${html_outfile}.cmd; \
	  sh $(outdir)/$${html_outfile}.cmd; \
	  if $(success_cmd) $(outdir)/$${html_outfile}; then \
	      echo "* SUCCESS"; \
	  elif $(failure_cmd) $(outdir)/$${html_outfile}; then \
	      (echo "*** FAILURE"; exit 1); \
	  else echo "*** UNKNOWN RESULT"; \
	  fi
endef

# Send an SQL command to myboss.
# Commas need to be entered with a variable $(comma).
comma := ,
boss_sql = echo "$(strip $(1));" | ssh $(boss) mysql tbdb

#================================================================
# activate: Set up the newly swapped-in EinE site to turn on as many forms as we can.
activate_tasks = new_proj1 new_proj2 approve_proj new_group \
	  new_user1 confirm_user1 approve_user1 \
	  new_user2 confirm_user2 \
	  new_exp1 new_exp2 mod_exp2
.PHONY:   $(activate_tasks)
activate: $(activate_tasks)

# Start two test projects.
# Leave testproj2 unapproved so approveproject_form.php3 remains active.
proj_common = $(call fmt_html_args,\
	    formfields[proj_URL]=$(root)\
	    formfields[proj_funders]=none\
	    formfields[proj_linked]=checked\
	    formfields[proj_members]=1\
	    formfields[proj_pcs]=3\
	    formfields[proj_public]=checked\
	    formfields[proj_why]=Test\ for\ injection.\
	    formfields[proj_whynotpublic]=\
	    submit=Submit)
proj1 = testproj
new_proj1: admin
	$(call wget_post,,newproject.php3,newproj1.html,\
	    formfields[pid]=$(proj1)\
	    formfields[proj_name]=$(proj1)\ description.\
	    $(proj_common))
# Assume we stay logged in and toggled to admin.
# Otherwise, each time looks like a separate browser login in the DB.
proj2 = testproj2
new_proj2:
	$(call wget_post,,newproject.php3,newproj2.html,\
	    formfields[pid]=$(proj2)\
	    formfields[proj_name]=$(proj2)\ description.\
	    $(proj_common))

# Approve one test project.  (Takes a couple of minutes to run.)
approve_proj:
	$(call wget_post,,approveproject.php3,,\
	    pid=$(proj1) approval=approve OK=Submit\
	    head_uid= user_interface=emulab pcplab_okay=Yep ron_okay=Yep message=)

# Create a subgroup of the test project.  (Takes a few seconds to run.)
new_group:
	$(call wget_post,,newgroup.php3,,\
	    group_pid=$(proj1) group_id=testgroup group_leader=$(uid) \
	    group_description=$(proj1)\ subgroup.)

# Make new users.  (Use http, not https; not logged in, join a project.)
# Do testuser and testusr2.
# Leave testusr2 unapproved (but confirmed) so approveuser_form.php3 is active.
user_common = $(call fmt_html_args,\
	    formfields[usr_title]=Tester\
	    formfields[usr_affil]=Emulab\ Scripts\
	    formfields[usr_URL]=http://www.emulab.net\
	    formfields[usr_addr]=Silly\
	    formfields[usr_addr2]=Address\
	    formfields[usr_city]=Salt\ Lake\ Silly\
	    formfields[usr_state]=UT\
	    formfields[usr_zip]=12345\
	    formfields[usr_country]=USA\
	    formfields[usr_phone]=(801)\ 123-4567\
	    formfields[password1]=user_pswd\
	    formfields[password2]=user_pswd\
	    formfields[pid]=$(proj1)\
	    formfields[gid]=\
	    submit=Submit)
usr1 = testuser
name1 = Test User 1
new_user1: logout
	$(call wget_post,$(root),joinproject.php3,newuser1.html,\
	    formfields[joining_uid]=$(usr1)\
	    formfields[usr_name]=$(subst $(empty) ,\ ,$(name1))\
	    formfields[wikiname]=$(subst $(empty) ,,$(name1))\
	    formfields[usr_email]=$(subst $(empty) ,,$(name1))@$(email_dest)\
	    $(user_common))
# Pretend the user sent in his e-mail confirmation.
confirm_user1:
	$(call boss_sql,\
	    update users set status='unapproved' where uid='$(usr1)')
query_user1:
	$(call boss_sql,\
	    select uid$(comma) usr_name$(comma) status \
	      from users where uid='$(usr1)')
usr2 = testusr2
name2 = Test User 2
new_user2: logout
	$(call wget_post,,joinproject.php3,newuser2.html,\
            formfields[joining_uid]=$(usr2)\
            formfields[usr_name]=$(subst $(empty) ,\ ,$(name2))\
            formfields[wikiname]=$(subst $(empty) ,,$(name2))\
            formfields[usr_email]=never.confirm@nowhere.net\
	    $(user_common))
# Pretend the user sent in his e-mail confirmation.
confirm_user2:
	$(call boss_sql,\
	    update users set status='unapproved' where uid='$(usr2)')
query_user2:
	$(call boss_sql,\
	    select uid$(comma) usr_name$(comma) status \
	      from users where uid='$(usr2)')

# Approve a new user.  (Takes a couple of minutes to run.)  (%24 is "$" .)
# Gotta log back in after being logged out above.
approve_user1: ###admin
	$(login_sys)
	$(admin_cmd)
	$(call wget_post,,approveuser.php3,appusr1.html,\
	    $(usr1)%24%24approval-$(proj1)%2F$(proj1)=approve\
	    $(usr1)%24%24trust-$(proj1)%2F$(proj1)=local_root\
            OK=Submit)

# Make an experiment via ssh and leave it not swapped in.
# (Do exp2 before exp1 so exp2 can finish creating before we do modifyexp on it.)
exp2 = testexp2
new_exp2:
	ssh $(ops) 'startexp -f -E "$(exp2) experiment." \
	    -p $(EinE_proj) -e $(exp2) shaped-2-nodes.ns'

# Make another new experiment.  Takes a few minutes to swap in.
# Must have at least one delay node for Traffic Shaping and Link Tracing pages.
exp_common =  $(call fmt_html_args,\
	    beginexp=Submit\
	    formfields[exp_pid]=$(EinE_proj)\
	    formfields[exp_gid]=\
	    MAX_FILE_SIZE=512000\
	    formfields[exp_localnsfile]=/users/$(USER)/shaped-2-nodes.ns\
	    formfields[exp_swappable]=1\
	    formfields[exp_noswap_reason]=\
	    formfields[exp_idleswap]=0\
	    formfields[exp_idleswap_timeout]=4\
	    formfields[exp_noidleswap_reason]=Because\
	    formfields[exp_autoswap]=0\
	    formfields[exp_autoswap_timeout]=16\
	    formfields[exp_linktest]=3)
exp1 = testexp1
new_exp1:
	scp -p $(SRCDIR)/shaped-2-nodes.ns $(ops):
	$(call wget_post,,beginexp_html.php3,newexp1.html,\
	    formfields[exp_id]=$(exp1)\
	    formfields[exp_description]=$(exp1)\ experiment.\
	    $(exp_common))

# Modify an experiment (first time creates an archive.)
mod_exp2:
	$(call wget_post,,modifyexp.php3,modexp2.html,\
	    pid=$(EinE_proj) eid=$(exp2) go=1\
	    MAX_FILE_SIZE=512000\
	    formfields[exp_localnsfile]=/users/$(USER)/shaped-2-nodes.ns\
	    reboot=1\
	    eventrestart=1)

#================================================================
# spider: Recursively a copy of the EinE site with wget and extract its forms list.
#
# Actually, spider it twice, once not logged in for the public view,
# and again, logged in and with administrative privs for the private view.
#
# The object here is to find and scan the forms, not execute them.

spider_tasks = clear_wget_dirs do_spider site_list site_msg
.PHONY: $(spider_tasks)
spider: $(spider_tasks)

# Login/admin mode changes are handled explicitly in the "activate:" target, and
# as "!actions" in the {setup,teardown}_forms.list specs controlling sep-urls.gawk .
# Don't follow page links that change the login/admin state.
#
# Also reject other links to pages which don't have any input fields, and don't ask
# for confirmation before taking actions.
top_links	= login.php3,logout.php3,toggle.php
user_links	= suuser.php,sendtestmsg.php3
exp_links	= showlogfile.php3,request_idleinfo.php3,request_swapexp.php3
node_links	= nodetipacl.php3,showconlog.php3,nodessh.php3
linkmon_links	= spewevents.php,linkmon_mon.php3 
rej_links	= \
	.txt,$(top_links),$(user_links),$(exp_links),$(node_links),$(linkmon_links)

# Clear out the wget directories.
.PHONY:          activate.wget public.wget admin.wget
clear_wget_dirs: activate.wget public.wget admin.wget
activate.wget:
	- rm -rf activate.wget.prev
	- mv -f activate.wget activate.wget.prev
	mkdir activate.wget
public.wget:
	- rm -rf public.wget.prev
	- mv -f public.wget public.wget.prev
	mkdir public.wget
admin.wget:
	- rm -rf admin.wget.prev
	- mv -f admin.wget admin.wget.prev
	mkdir admin.wget

# Finally ready to grab the whole site.
.PHONY:    public_spider admin_spider
do_spider: public_spider admin_spider

public_spider: logout public.wget/public.log
public.wget/public.log: 
	$(logout_cmd)
	cd public.wget; \
	wget -r -S $(cookie_args) $(ld_cookies_subdir) -o public.log \
	     -k -D $(dom) -R $(rej_links) -X /downloads,/gallery $(sroot)
	du -s public.wget

admin_spider: admin.wget/admin.log
admin.wget/admin.log:
	$(login_sys)
	$(admin_cmd)
	@echo "** Be patient, spidering will take about 10 minutes. **"
	cd admin.wget; \
	wget -r -S $(cookie_args) $(ld_cookies_subdir) -o admin.log \
	     -k -D $(dom) -R $(rej_links) -X /downloads,/gallery $(sroot)
	du -s admin.wget

# Extract a list of the active forms in the site.
SITE_FORMS	= $(RESDIR)/site_forms.list
SITE_FILES	= $(RESDIR)/site_files.list
site_list: $(SITE_FORMS) $(SITE_FILES)

# Ignore flyspray and Twiki for now.
# Ignore the search box form on every page, we'll treat it separately.
FORMS_CMD = find . \( -name distributions -prune \) \
		-o \( -name flyspray -prune \) \
		-o \( -name twiki -prune \) \
		-o -type f -print0 | xargs -0 grep -n '<form ' | \
	    fgrep -v /search.php3 ) | sort -u
# Filenames - Remove directory prefix and "Get" arg lists after the filename.
# Kill suffix after filename first: .../archive_view.php3/9/trunk?exptidx=9
FILES_CMD = sed -e 's|\(php3*\).*|\1|' -e 's|^[^:]*/||' 

PUBLIC_FORMS	= $(RESDIR)/public_forms.list
PUBLIC_FILES	= $(RESDIR)/public_files.list
public_list: $(PUBLIC_FORMS) $(PUBLIC_FILES)
$(PUBLIC_FORMS): public.wget/public.log
	(cd public.wget; $(FORMS_CMD) > $(PUBLIC_FORMS)
$(PUBLIC_FILES): $(PUBLIC_FORMS)
	$(FILES_CMD) $(PUBLIC_FORMS) | uniq > $(PUBLIC_FILES)

ADMIN_FORMS	= $(RESDIR)/admin_forms.list
ADMIN_FILES	= $(RESDIR)/admin_files.list
admin_list: $(ADMIN_FORMS) $(ADMIN_FILES)
$(ADMIN_FORMS): admin.wget/admin.log
	(cd admin.wget; $(FORMS_CMD) > $(ADMIN_FORMS)
$(ADMIN_FILES): $(ADMIN_FORMS)
	$(FILES_CMD) $(ADMIN_FORMS) | uniq > $(ADMIN_FILES)

$(SITE_FORMS): $(PUBLIC_FORMS) $(ADMIN_FORMS)
	cat $(PUBLIC_FORMS) $(ADMIN_FORMS) | sort -u > $(SITE_FORMS)
# The <forms under index.html are actually in menu.php3 via defs.php3.in .
# Ditto beginexp_{html,form}.php3 .
$(SITE_FILES): $(PUBLIC_FILES) $(ADMIN_FILES)
	cat $(PUBLIC_FILES) $(ADMIN_FILES) | \
	    sed -e 's/index\.html/menu.php3/' \
		-e 's/beginexp_html/beginexp_form/' | sort -u > $(SITE_FILES)

site_msg: site_list public_list admin_list
	@echo "** `wc -l < $(SITE_FORMS)` (`wc -l < $(PUBLIC_FORMS)` +" \
	      "`wc -l < $(ADMIN_FORMS)` ) forms instances" \
	      "are in `wc -l < $(SITE_FILES)` (` wc -l < $(PUBLIC_FILES)` +" \
	      "`wc -l < $(ADMIN_FILES)` ) web pages.  **" | tr -s " "

#================================================================
# forms_coverage: Compare the two lists to find uncovered (unlinked) forms.
.PHONY:         files_missing forms_msg
forms_coverage: files_missing forms_msg

FILES_MISSING = $(RESDIR)/files_missing.list
files_missing: $(FILES_MISSING)
$(FILES_MISSING): $(SRC_FILES) $(SITE_FILES)
	diff $(SRC_FILES) $(SITE_FILES) | grep '^[<>]' > $(FILES_MISSING)

forms_msg: files_missing src_msg site_msg
	@echo "** `wc -l < $(FILES_MISSING)` forms files are not covered.  **" \
		| tr -s " "

# Look at files_missing.list and see README-howto.txt for the
# procedure to activate coverage of more forms.

#================================================================
# input_coverage: Grep spidered forms for <input definitions, make values dictionary.
.PHONY:         input_list input_msg
input_coverage: input_list input_msg

SITE_INPUTS     = $(RESDIR)/site_inputs.list
INPUT_NAMES	= $(RESDIR)/input_names.list
input_list: $(SITE_INPUTS) $(INPUT_NAMES)
PUBLIC_INPUTS   = $(RESDIR)/public_inputs.list
ADMIN_INPUTS    = $(RESDIR)/admin_inputs.list
$(SITE_INPUTS): $(PUBLIC_INPUTS) $(ADMIN_INPUTS)
	cat $(PUBLIC_INPUTS) $(ADMIN_INPUTS) > $(SITE_INPUTS)

# Extract input fields and context from the html form files in the wget subdirs.
# Output sections terminated by a blank line contain: filename, <form, <input* .
# Canonicalize and reorder: <input type="..." name="..." value=... ...>
# INPUTS_CMD is parameterized by dir:={public,admin}
DIR_FORMS 	= $(RESDIR)/$(dir)_forms.list
DIR_INPUTS	= $(RESDIR)/$(dir)_inputs.list
# This awk command gets a long list of forms file names on the command line.
INPUTS_CMD = @echo "form-inputs from $(dir)_forms.list files to $(dir)_inputs.list"; \
	(cd $(dir).wget && gawk -f ../$(SRCDIR)/form-input.gawk \
	     $(shell sed -e "s/:[0-9][0-9]*:.*//" -e "s/.*/'&'/" $(DIR_FORMS) ) \
	     > ../$(DIR_INPUTS) )
dir := public
PUBLIC_INPUTS_CMD := $(INPUTS_CMD)
$(PUBLIC_INPUTS): $(PUBLIC_FORMS) $(SRCDIR)/form-input.gawk
	$(PUBLIC_INPUTS_CMD)
dir := admin
ADMIN_INPUTS_CMD := $(INPUTS_CMD)
$(ADMIN_INPUTS): $(ADMIN_FORMS) $(SRCDIR)/form-input.gawk
	$(ADMIN_INPUTS_CMD)

# Get unique input field names: text(area), hidden, checkbox, select, radio/checked.
$(INPUT_NAMES): $(SITE_INPUTS)
	gawk '/type="(text|hidden|checkbox|select)/ || /\<checked\>/ \
	    {print $$3}' $(SITE_INPUTS) | sort -u > $(INPUT_NAMES)

input_msg: input_list
	@echo "** `grep -c '<input' $(SITE_INPUTS)` input fields," \
	      "`wc -l < $(INPUT_NAMES)` unique.  **" | tr -s " "

# Copy input_names.list to input_values.list at first, 
# then edit default values onto the lines for auto-form-fill-in.
INPUT_VALUES	= $(SRCDIR)/input_values.list

#================================================================
# normal: Create and run "normal operations" test cases.
#
# Convert the input list to normal test cases with input field values.
# Test until "normal" input tests work properly on all forms.
# 
normal: gen_all run_all
gen_tasks = gen_setup gen_normal gen_teardown
run_tasks = run_setup run_normal run_teardown analyze
.PHONY: gen_all run_all $(gen_tasks) $(run_tasks)
gen_all: $(gen_tasks)
run_all: $(run_tasks)

NORMAL_URLS	= $(RESDIR)/site_normal.urls
NORMAL_WGET	= $(RESDIR)/normal_cases.wget
NORMAL_CASES	= $(RESDIR)/normal_cases.xml
gen_normal: $(NORMAL_URLS) $(NORMAL_WGET) ###$(NORMAL_CASES)

# Separate out the setup and teardown URL's from the normal ones.
SETUP_URLS	= $(RESDIR)/site_setup.urls
SETUP_WGET	= $(RESDIR)/setup_cases.wget
SETUP_CASES	= $(RESDIR)/setup_cases.xml
gen_setup: $(SETUP_URLS) $(SETUP_WGET) ###$(SETUP_CASES)

TEARDOWN_URLS	= $(RESDIR)/site_teardown.urls
TEARDOWN_WGET	= $(RESDIR)/teardown_cases.wget
TEARDOWN_CASES	= $(RESDIR)/teardown_cases.xml
gen_teardown: $(TEARDOWN_URLS) $(TEARDOWN_WGET) ###$(TEARDOWN_CASES)

sep_src		= $(SRCDIR)/sep-urls.gawk
sep_cmd		= gawk -f $(sep_src) -v SYSADMIN=$(uid)
f2u_src		= $(SRCDIR)/forms-to-urls.gawk
u2w_src		= $(SRCDIR)/urls-to-wget.gawk

SETUP_FORMS	= $(SRCDIR)/setup_forms.list
TEARDOWN_FORMS	= $(SRCDIR)/teardown_forms.list

$(NORMAL_URLS) $(SETUP_URLS) $(TEARDOWN_URLS): \
  $(f2u_src) $(SITE_INPUTS) $(INPUT_NAMES) $(INPUT_VALUES) \
  $(sep_src) $(SETUP_FORMS) $(TEARDOWN_FORMS)
	gawk -f $(f2u_src) -v VALUES=$(INPUT_VALUES) \
            $(SITE_INPUTS) > tmp_urls
	fgrep -v -f $(SETUP_FORMS) tmp_urls | \
	    fgrep -v -f $(TEARDOWN_FORMS) > $(NORMAL_URLS)
	$(sep_cmd) $(SETUP_FORMS) tmp_urls > $(SETUP_URLS)
	$(sep_cmd) $(TEARDOWN_FORMS) tmp_urls > $(TEARDOWN_URLS)

# WebInject doesn't store the returned pages.  Use wget and browse the directory.
$(NORMAL_WGET): $(NORMAL_URLS) $(u2w_src)
	gawk -f $(u2w_src) $(NORMAL_URLS) > $(NORMAL_WGET)
$(SETUP_WGET): $(SETUP_URLS) $(u2w_src)
	gawk -f $(u2w_src) $(SETUP_URLS) > $(SETUP_WGET)
$(TEARDOWN_WGET): $(TEARDOWN_URLS) $(u2w_src)
	gawk -f $(u2w_src) $(TEARDOWN_URLS) > $(TEARDOWN_WGET)

# XML test cases for WebInject.
$(NORMAL_CASES): $(NORMAL_URLS) $(SRCDIR)/urls-to-webinject.gawk
	gawk -f $(SRCDIR)/urls-to-webinject.gawk $(NORMAL_URLS) > $(NORMAL_CASES)

# Test until "normal" input tests work properly in all forms.
run_setup: $(SETUP_WGET) gen_setup
	csh -f $(SETUP_WGET)
run_normal: $(NORMAL_WGET) ###run_setup
	csh -f $(NORMAL_WGET)
run_teardown: $(TEARDOWN_WGET) run_setup
	csh -f $(TEARDOWN_WGET)

analyze_output = analyze_output.txt
tee = | tee >> $(analyze_output)
##cut = | cut -c -80
analyze_hdr = echo ================  $(1)  ================ $(tee)
analyze_cmd = fgrep -H -f $(SRCDIR)/$(1) *.html | tr -s " " $(cut) $(tee)
analyze:
	@echo > $(analyze_output)
	@$(call analyze_hdr,success)
	-$(call analyze_cmd,success.txt)
	@$(call analyze_hdr,failure)
	-$(call analyze_cmd,failure.txt)
	@$(call analyze_hdr,UNKNOWN)
	@sed -n 's/:.*//p' $(analyze_output) | uniq > recognized_output.files
	ls *.html | fgrep -v -f recognized_output.files $(tee)

NORMAL_OUTPUT	= $(RESDIR)/normal_output.xml
run_webinject: $(NORMAL_OUTPUT)
$(NORMAL_OUTPUT): $(NORMAL_CASES)
	(cd $(SRCDIR)/webinject;
	    webinject.pl ../$(NORMAL_CASES);
	    mv results.xml ../$(NORMAL_OUTPUT)

#================================================================
# probe: Create and run probes to test the checking code of all input fields.
.PHONY: gen_probes run_probes
probe:  gen_probes run_probes

# Generate labeled mock SQL injection probes in individual fields.
# Probe strings are labeled with the form and field names that caused the hole.
#
# These also need to be separated into setup, teardown, and normal, because of
# unrepeatable operations being caught by checking code, e.g. can't ask it to create a
# project when it already exists...  So the setup probing has to be done in the
# teardown state, and vice versa.
#
PROBE_URLS	= $(PROBE_SETUP_URLS) $(PROBE_TEARDOWN_URLS) $(PROBE_NORMAL_URLS)
PROBE_SETUP_URLS = $(RESDIR)/setup_probe.urls
PROBE_TEARDOWN_URLS = $(RESDIR)/teardown_probe.urls
PROBE_NORMAL_URLS = $(RESDIR)/normal_probe.urls
#
PROBE_WGET	= $(PROBE_SETUP_WGET) $(PROBE_TEARDOWN_WGET) $(PROBE_NORMAL_WGET)
PROBE_SETUP_WGET = $(RESDIR)/probe_setup.wget
PROBE_TEARDOWN_WGET = $(RESDIR)/probe_teardown.wget
PROBE_NORMAL_WGET = $(RESDIR)/probe_normal.wget

gen_probes: $(PROBE_URLS) $(PROBE_WGET)
$(PROBE_URLS): \
  $(f2u_src) $(SITE_INPUTS) $(INPUT_NAMES) $(INPUT_VALUES) \
  $(sep_src) $(SETUP_FORMS) $(TEARDOWN_FORMS)
	gawk -f $(f2u_src) -v PROBE=1 -v VALUES=$(INPUT_VALUES) \
            $(SITE_INPUTS) > tmp_probe_urls
	fgrep -v -f $(SETUP_FORMS) tmp_probe_urls | \
	    fgrep -v -f $(TEARDOWN_FORMS) > $(PROBE_NORMAL_URLS)
	$(sep_cmd) $(SETUP_FORMS) tmp_probe_urls > $(PROBE_SETUP_URLS)
	$(sep_cmd) $(TEARDOWN_FORMS) tmp_probe_urls > $(PROBE_TEARDOWN_URLS)

# Output to a subdir, cookies.txt file in the parent dir.
OUT_ARG = -v OUTDIR=probes.wget
$(PROBE_NORMAL_WGET): $(PROBE_NORMAL_URLS) $(u2w_src)
	gawk -f $(u2w_src) $(OUT_ARG) $(PROBE_NORMAL_URLS) > $(PROBE_NORMAL_WGET)
$(PROBE_SETUP_WGET): $(PROBE_SETUP_URLS) $(u2w_src)
	gawk -f $(u2w_src) $(OUT_ARG) $(PROBE_SETUP_URLS) > $(PROBE_SETUP_WGET)
$(PROBE_TEARDOWN_WGET): $(PROBE_TEARDOWN_URLS) $(u2w_src)
	gawk -f $(u2w_src) $(OUT_ARG) $(PROBE_TEARDOWN_URLS) > $(PROBE_TEARDOWN_WGET)

probe_all: gen_probes probe_setup probe_teardown probe_normal probes_msg
.PHONY: probe_all probe_setup probe_teardown probe_normal probes_analyze probes_msg
probe_setup: $(PROBE_SETUP_WGET)
	csh -f $(PROBE_SETUP_WGET)
probe_normal: $(PROBE_NORMAL_WGET) ###probe_setup
	csh -f $(PROBE_NORMAL_WGET)
probe_teardown: $(PROBE_TEARDOWN_WGET) probe_setup
	csh -f $(PROBE_TEARDOWN_WGET)

PROBE_LABELS = $(RESDIR)/probe-labels.list
UNCAUGHT_PROBES = $(RESDIR)/uncaught-probes.list
UNCAUGHT_FILES = $(RESDIR)/uncaught-files.list
probes_analyze:
	cd probes.wget; \
	gmake SRCDIR=../$(SRCDIR) -f ../GNUmakefile analyze
probes_msg: probes_analyze
	@cat probes.wget/*.html | fgrep 'Probe label:' > $(PROBE_LABELS)
	@fgrep ": '" < $(PROBE_LABELS) | sort > $(UNCAUGHT_PROBES)
	@sed 's/.*{\([^:]*\).*/\1/' $(UNCAUGHT_PROBES) | uniq > $(UNCAUGHT_FILES)
	@echo "** "`wc -l < $(PROBE_NORMAL_WGET)`" probes to " \
	      `wc -l < $(NORMAL_URLS)`" pages gave" \
	      `wc -l < $(PROBE_LABELS)`" hits: " \
              `fgrep -c ': \' < $(PROBE_LABELS)`" backslashed, " \
	      `wc -l < $(UNCAUGHT_PROBES)`" uncaught in " \
	      `wc -l < $(UNCAUGHT_FILES)`" pages." | tr -s " "

# Run the probes through webinject.
# Successfully caught cases should produce "invalid input" warnings.
# Potential penetrations will log SQL errors with the form/field name.
PROBE_OUTPUT	= $(RESDIR)/probe_output.xml
###run_probes: $(PROBE_OUTPUT)
$(PROBE_OUTPUT): $(PROBE_CASES)
	(cd $(SRCDIR)/webinject;
	   webinject.pl ../$(PROBE_CASES);
	   mv results.xml ../$(PROBE_OUTPUT)
