<!--
   EMULAB-COPYRIGHT
   Copyright (c) 2000-2008 University of Utah and the Flux Group.
   All rights reserved.
  -->
<center>
<h1>Hardware Recommendations</h1>
</center>

<h2>Contents</h2>
<ul>
<li> <a href="#PURP">Purpose of this document</a>
<li> <a href="#NODES">Nodes</a>
<li> <a href="#SWITCHES">Switches</a>
<li> <a href="#SERVERS">Servers</a>
</ul>

<hr>

<a NAME="PURP"></a><h2>Purpose of this document</h2>
The purpose of this document is to share our experience in selecting the
hardware in Emulab, to aid others who are considering similar projects. Rather
than a recipe for building testbeds, this document gives a set of
recommendations, and outlines the consequences of each choice.

<hr>

<a NAME="NODES"></a><h2>Nodes</h2>
<dl>

	<dt><b>NICs</b></dt>
		<dd>At least 2 - one for control net, one for the experimental
		network.  Our attempts to use nodes with only 1 NIC have not
		met with much success. 3 interfaces lets you have delay nodes,
		but only linear topologies (no branching) unless you
		"multiplex" the physical links. 4 lets you have
		(really simple) routers. We opted to go with 5.
		The control net
		interface should have PXE capability and need only be 100Mb.
		All experimental
		interfaces should be the same, unless you are purposely going
		for a heterogenous environment. We use Intel cards (Pro100,
		Pro1000) almost exclusively, but others have also used
		Broadcom, 3Com and D-link interfaces.
		The control net interface can
		be different.  For the control net we have used
		Intel Pro100 (fxp) and Pro1000 (em) cards,
		as well as builtin Broadcom (bge), 3Com (xl) and RealTek (re)
		devices.
		Note that the older the card/motherboard, the more likely
		it is that either it will not have PXE or that the PXE
		implementation will have bugs, so it's
		best to get a sample first and try it out before committing to
		a card. Newer (say 2003 or newer) cards will most likely
		have a correctly functioning PXE.
		Depending on usage, it may be OK to get a large number
		of nodes with 2 interfaces to be edge nodes, and a smaller
		number with more interfaces to be routers.</dd>

	<dt><b>Case/Chassis</b></dt>
		<dd>This will depend on your space requirements. The cheapest
		option is to buy standard desktop machines, but these are not
		very space-efficient. 3U or 4U rackmount cases generally have
		plenty of space for PCI cards and CPU cooling fans, but still
		may consume too much space for a large-scale testbed.  Smaller
		cases (1U or 2U) have fewer PCI slots (usually 2 for 2U cases,
		and 1 or 2 in 1U cases), and often require custom motherboards
		and/or "low profile" PCI cards.
		Heat is an issue in smaller cases, as they may not have room
		for CPU fans. This limits the processor speed that can be used.
		Smaller cases also make for denser wiring.  See
		<a href="/gallery/clusterI/full/firstrack.jpg">the back of one of our pc850 racks</a>
		for example.
		Denser cabling can affect air flow exacerbating heat issues.
		For our first round of machines ("pc600"), we bought standard
		motherboards and 4U cases, and assembled the machines ourselves.
		For our second round of PCs ("pc850"), we opted for the (now
		discontinued) <a
		href="http://www.intel.com">Intel</a> ISP1100 server platform,
		which includes a 1U case, custom motherboard with 2 onboard
		NICs and serial console redirection, and a power supply.
		For the third round of PCs ("pc3000") we got <a
		href="http://www1.us.dell.com/content/products/productdetails.aspx/pedge_2850">Dell 2850s</a> which sport 2U cases and a custom motherboard
		with 2 builtin NICs and serial console redirection.</dd>

	<dt><b>CPU</b></dt>
		<dd>Take your pick. Note that small case size (ie. 1U) may
		limit your options, due to heat issues. Many experiments will
		not be CPU bound, but some uses (ie. cluster computing
		experiments) will appreciate fast CPUs.  Are latest round
		of machines have 3Ghz processors.</dd>

	<dt><b>Memory</b></dt>
		<dd>While many applications don't need much, memory is still
		cheap and you should get at least 512MB, and preferably
		1-2GB.  We chose to go with ECC, since it is not much more
		expensive than non-ECC, and with our large scale, ECC will help
		protect against failure.</dd>

	<dt><b>Motherboard</b></dt>
		<dd>Serial console redirection is nice, and the BIOS should
		have the ability to PXE boot from a card or a builtin
		interface.
		Health monitoring hardware (temperature,
		fans, etc.) is good too, but not required.
		If you are planning to have 1 or more 1000Mb interfaces you
		should be looking for PCI-X or PCI Express busses.
		Onboard network interfaces can allow you get get more NICs,
		something especially valuable for small cases with a limited
		number of expansion slots.</dd>

	<dt><b>Hard Drive</b></dt>
		<dd>Pretty much any one is OK. With a large number of nodes,
		you are likely to run into failures, so they should be
		reasonably reliable.  As for size, whatever the current
		market "sweet spot" is should be more than adequate.
		Our standard images are at most 6GB.  Consider SCSI if
		you have money and need speed.  Consider a second drive
		if you have money and case room, and need a whole lot
		of space for logging, etc.</dd>

	<dt><b>Floppy</b></dt>
		<dd>Handy for BIOS updates, diagnostics and possibly for
		older OS installs.
		You definitely need one if you don't have a CDROM drive.</dd>

	<dt><b>CDROM</b></dt>
		<dd>Recommended if you might grow your testbed large
		(more than a couple hundred machines) some day.
		For reliable scaling we may someday migrate away
		from PXE-booting to CD-based booting,
		as we already do with our widearea and wireless nodes.</dd>

	<dt><b>USB</b></dt>
		<dd>We currently do not use any USB devices, though booting
		from a write-protected USB flash drive is another alternative
		to booting via PXE.</dd>

	<dt><b>VGA</b></dt>
		<dd>Only needed if motherboard does not have serial console
		redirection or if you want to run Windows.
		Cheap is fine, cheap and builtin is better.</dd>
</dl>

<hr>

<a NAME="SWITCHES"></a><h2>Switches</h2>
<dl>
	<dt><b>Control net</b></dt>
		<dd><dl>

		<dt><b>Number of ports</b></dt>
			<dd>You'll need one port for each testbed node, plus
			ports for control nodes, power controllers, etc.
			100Mb ports are fine.</dd>

		<dt><b>VLANs</b></dt>
			<dd>Allows partitioning of the control network,
			which protects control hw (switches, power, etc.) from
			nodes and world, and private machines. Without them,
			control hardware can be attached to an additional NIC
			on the boss node (requires extra cheap switch.)</dd>

		<dt><b>Router</b></dt>
			 <dd>Required if VLANs are used. Must have DHCP/bootp
			 forwarding.  (Cisco calls this the 'IP Helper') A MSFC
			 card in a Cisco Catalyst supervisor module works well
			 for us, but a PC would probably suffice.</dd>

		 <dt><b>Firewall</b></dt>
			 <dd>Can be the router, without it VLAN security is
			 pretty much useless, meaning that it may be possible
			 for unauthorized people to reboot nodes and configure
			 experimental network switches.</dd>

		 <dt><b>Port MAC security</b></dt>
			 <dd>Helps prevent nodes from impersonating each other
			 and control nodes. This is an unlikely attack, since
			 it must be attempted by an experimenter or someone who
			 has compromised an experimental node already.</dd>

		 <dt><b>Multicast</b></dt>
			 <dd>Switch multicast support (IGMP snooping) and
			 router (if present) is used to allow multicast loading
			 of disk images.  Otherwise, they must be done unicast
			 and serially, which is an impediment to re-loading
			 disks after experiments (to return nodes to a clean
			 state.)</dd>

		 </dl>

	<dt><b>Experimental net</b></dt>
		 <dd><dl>

		 <dt><b>Vendor</b></dt> <dd>Our software currently supports:
		 Cisco Catalyst 65xx, 40xx, 29xx, and 55xx series,
		 Nortel 1100 and 5510,
		 Foundry 1500 and 9604,
		 and Intel 510T
		 switches (although the Intels don't support everything you'd
		 want for a large-scale production testbed).  Other
		 switches by the same vendors will likely be easy to support -
		 in particular, we expect that Cisco switches supporting
		 the 'CISCO-VTP-MIB', and the 'CISCO-STACK-MIB' or 
		 'CISCO-VLAN-MEMBERSHIP-MIB' are likely to 'just work'.
		 (You can see a list of which MIBs are
		 supported by which devices at <a
		 href="http://www.cisco.com/public/sw-center/netmgmt/cmtk/mibs.shtml">Cisco's
		 MIB page.)</a> If you will be using multiple Ciscos for the
		 experimental net, and trunking between them, your switch
		 should also support the 'CISCO-PAGP-MIB', though this is not
		 required. Switches from other vendors can theoretically be
		 used if they support SNMP for management, but will likely
		 require significant work. We have found that you can often
		 find good deals on new or used equipment on EBay.
		 For used Cisco equipment, we buy from <a
		 href="http://www.atantica.com/">Atantica</a>.</dd>

		 <dt><b>Number of ports</b></dt>
			 <dd>You'll need as many ports as you have experimental
			 interfaces on your nodes. In addition, you need 1 port
			 to connect the experimental net switch to the control
			 net (for SNMP configuration.) If you're using multiple
			 switches, you need sufficient ports to 'stack' them
			 together - If your switches are 100Mbit, Gigabit ports
			 are useful for this.</dd>

		 <dt><b>VLAN support</b></dt>
			<dd>Optimally, configurable via SNMP (we have no tools
			to configure it otherwise.) If not available, all
			experimental isolation is lost, delay nodes can't be
			used well, and method for coming up with globally
			unique IP addresses will be required. So, VLANs are
			basically necessary.</dd>

		<dt><b>Trunking</b></dt>
			<dd>If multiple switches, should have a way to trunk
			them (or experiments are limited to 1 switch.) Ideally,
			all trunked switches should support VLAN management
			(like Cisco's VTP) and a VLAN trunking protocol like
			802.1q .  It's good if the trunk links are at least an
			order of magnitude faster than node links, and link
			aggregation (ie. EtherChannel) is desirable.</dd> </dl>
			</dl>

<hr>

<a NAME="SERVERS"></a><h2>Servers</h2>
	Two is preferable, though one could be made to work if you are willing
	to invest some time and effort. The NFS server
	needs enough space for /users and /proj , as well as a few extra gigs
	for build trees, logs, and images.  If you have more than 128 nodes,
	and plan to use the RocketPort serial ports, you need one
	"tip server" machine per 128 serial lines (other serial muxes may
	have similar limitations.)  Tip servers do not need to be very
	powerful, we use old desktop machines that have full-height PCI slots.
        A 1000Mb network connection is suggested for the disk image
	distribution machine (usually boss.) though you can get by with 100Mb.
	The database machine (boss) should have reasonably fast CPU and
	plenty of RAM.

<hr>

<a NAME="OTHER"></a><h2>Other Hardware</h2>
<dl>
	<dt><b>Network cables</b></dt>
		<dd>We use Cat5E, chosen because they are not much more
		expensive than Cat5, and can be used in the future for gigabit
		Ethernet. It has been our experience that 'boots' on cables do
		more harm than good. The main problems are that they make it
		difficult to disconnect the cables one connected, and that they
		get in the way on densely-connected switches. Cables with
		'molded strain relief' are better than cables with boots, but
		are often much more extensive. We buy cables in two-foot
		increments, which keeps slack low without making the order too
		complicated. Our "standard" so far has been to make control net
		cables red, experimental net cables yellow, serial cables
		white, and cables for control hardware (such as power
		controllers) green.  We've bought all of our cables from <a
		href="http://www.dataaccessories.com">dataaccessories.com</a>,
		and have had excellent luck with them. With prices of $3.00 to
		$4.25 for premade cables, it's not too expensive to by them
		rather than make your own, and it's much easier.</dd>

	<dt><b>Serial cables</b></dt>
		<dd>We use Cat5E, but with a special pin pattern on the ends to
		avoid interference between the transmit/receive pairs. We use
		RJ-45 connectors on both ends, and a custom serial hood to
		connect to the DB-9 serial ports on the nodes. Unfortunately,
		the custom pinout is different for different serial mux
		manufacturers. Contact us to get our custom cable specs
		for RocketPort serial mux.</dd>

	<dt><b>Power controllers</b></dt>
		<dd>Without them, nodes can not be reliably rebooted. We
		started out with 8-port SNMP-controlled power controllers from
		<a href="http://www.apc.com">APC</a>. We then switched to
		using the
		RPC-27 from <a href="http://www.baytechdcd.com/">BayTech</a>,
		20-outlet vertically-mounted, serial-controlled power
		controllers. Currently, we use the BayTech rackmount RPC14
		series. The serial controllers are generally cheaper, and
		the more ports on each controller, the cheaper.
		Take note of the power requirements of your machines before
		buying power controllers!  While we can support 20 old 850Mhz
		machines on a single 30A/120V power controller, we can only
		support 7 Dell 2850s per 30A/120V controller.  So we went
		with 8-port controllers for the latter.</dd>

	<a NAME="SERIALMUX"></a>
	<dt><b>Serial (console) ports</b></dt>
		<dd>Custom kernels/OSes (specifically, the OSKit) may not
		support ssh, etc. Also useful if an experimenter somehow scrogs
		the network. We currently use the <a
		href="http://www.comtrol.com/products/rocketport.asp">
		Comtrol RocketPort</a> series of serial muxes.
		In particular, we use the 32-port RocketPort Universal PCI
		interfaces (pn 99356-8) and 1U rackmount panels (pn 99380-3).
		You can only have 4 cards (or 128 ports) per
		machine, so if you have more than 128 nodes, you will need
		multiple machines to host the interfaces.
		Note also that the 32-port RocketPort is a full-height PCI
		card, so make sure you have a machine with full-height
		(not low-profile) slots. (<em>In the past we have used the
		<a href="http://cyclades.com">Cyclades</a>
		Cyclom Ze serial adapters, which allow up to 128 serial ports
		in a single PC.  But those have been discontinued and they
		also require a 5V PCI slot</em>)
		</dd>

	<dt><b>IPMI and ASF support</b></dt>
		<dd>Most server motherboards, and some desktops, support
		some form of remote management including remote reset and
		even "Serial Over LAN."  To date, we have only experimented
		with ASF for power cycling on a particular Dell Optiplex box
		with a Broadcom controller.  Getting this working in a non-
		Windows environment required a Dell-mediated agreement with
		Broadcom to obtain a Linux configuration client and, in at
		least this instantiation, the ASF controller can be disabled
		(confused?) by the host OS driver.  So this has not been a
		cost-effective solution thus far.
		</dd>

	<dt><b>"Whack on LAN" power controllers</b></dt>
		<dd>One of our students developed a hardware modification to
		the Wake-on-LAN mechanism to allow us to reset machines
		via the network.  See<a
		href="http://www.cs.utah.edu/gradsac/rd2005/ayers.html">
		this abstract</a> for a little info.</dd>
</dl>
