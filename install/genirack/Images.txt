Notes on baking the images.

Each rack gets a backed image of the control node, and two baked
images for boss and ops. The control node image is discussed below,
but first lets talk about the boss/ops XEN VMs.

Briefly, the VMs are initially created as a XEN based ElabInElab
experiment using an NS file tailored to the eventual environment via a
bunch of attribute variables. For example, take a look at this one,
which is the basis for the BBN rack:

  https://www.emulab.net/showexp.php3?pid=testbed&eid=bbnrack#nsfile

The initial config lines turn on/off some Emulab features, but most
importantly causes the ProtoGeni subsystem to be configured in and the
packages loaded. The CONFIG_GENIRACK takes it further, running the
ProtoGeni initsite script, which will generate all of the PG
certificates and upload them to the Emulab website. When running in
Utah, the VMs will look just like they would if they were on the
remote network, except that we temporarily change things so that they
will actually boot on our network. Later just before we take the
snapshots of the VMs, we clean that stuff out; when they next boot,
they better be booting on the control node at the remote site!

The rest of the config variables are set according to the particulars
of the site, as told to us by the local site admin.

For each site, we need to create a directory with three files in it:

    ips.txt       - The ip addresses for the control/ilo interfaces
    ilo.xml       - The XML file of ilo info from HP
    variables.txt - Some extra config variables.

Take a look at the boss:~stoller/genirack/racks/bbn directory for
examples.  The variables file is a bunch of passwords, which are
random strings I generated by piping some bytes from /dev/random into
md5 and taking a substring. You can generate a new one of these files
by running:

    boss> ~stoller/genirack/racks/genpswds.pl > variables.txt

The ip.txt file is specific to the site of course, and the XML file
comes from HP via email.

Utah Emulab is going to function as a secondary for each rack's boss
named. So we need to add a secondary record to our named config file,
in both the internal and external views. Edit /etc/namedb/named.conf,
but be sure to "co -l" the file (make sure there are no changes to
the file first; people forget). Look for the BBN rack version as
an example.

	# Secondary for foo.net
	zone "foo.net" {
		type slave;
		# IP of rack boss
		masters { XXX.XXX.XXX.XXX; };
		file "slave/foo.net.db";
	};
	zone "129/25.242.1.192.in-addr.arpa" in {
	    type slave;
	    # IP of rack boss
	    masters { XXX.XXX.XXX.XXX; };
	    file "slave/reverse-foo.net.db";
	};

Utah Emulab is also the DNS server for the control node IPs. So in
/etc/namedb/instageni.net.db we need two entries. For example:

	gpolab.control-nodes		IN	A	192.1.242.130
	gpolab-ilo.control-nodes	IN	A	192.1.242.131

Be sure to change the serial number at the top of the file.

Now run named_setup to get named restarted on boss.

Swap the experiment in.

Once the VMs are ready, copy the directory mentioned above over to the
inner boss as /usr/testbed/etc/genirack. Do NOT put this stuff on the
inner ops!

Now we convert the VMs for boot in the target environment, which means
cleaning up some stuff and changing a bunch of things.  First ssh (as
root) into the inner boss VM from outer boss:

	boss> cd /usr/testbed/obj/testbed/install
	boss> sudo perl emulab-install -c -i boss/genirack boss

then log out and ssh (as root) into inner ops:

	ops> cd /usr/testbed/obj/testbed/install
	ops> sudo perl emulab-install -i ops/genirack ops

Grab /usr/testbed/etc/elabman.pswd and store it in our file. See
Utah boss:~stoller/genirack/pswd.txt. 

Now we have to shutdown the VMs. Log into the physical host and then:

	vhost-0> sudo /usr/local/etc/emulab/vnodesetup -jh pcvmXXX-1
	vhost-0> sudo /usr/local/etc/emulab/vnodesetup -jh pcvmXXX-2
	
The -h option is very important; it says to keep the disks intact.
If you forget that, you have to go back to the beginning and start
over.

Next step is to capture the entire state of the VMs, which includes an
imagezip of each lvm, a copy of the kernel, and a slightly modified
xm.conf file. The script that does this might not be installed, but
you can find it in the testbed source directory in
clientside/tmcc/linux/openvz.

	vhost-0> sudo /usr/local/etc/emulab/capturevm.pl -r boss pcvmXXX-1
	vhost-0> sudo /usr/local/etc/emulab/capturevm.pl -r ops pcvmXXX-2

This will take a little while of course. When finished, cd into
/capture and you will find two directories named boss and ops.
You want to create a tar file (no point in using compression).

	vhost-0> sudo tar cf foo.tar boss ops

Now copy the tar file over to the new control node. 

----

Control Node Image:

The control node image is currently baked from the Utah control
node. We have an extra disk on our control node that is a duplicate of
the main disk. Well, it was at one time, but we don't change it very
often and when I do, I try to remember to update the mirror as well.
Anyway, there are just a few things that need to be changed on the
control image for each site.

Note, DO NOT CHANGE THESE ON THE ROOT DISK! The clone is mounted on
/mnt and /mnt/usr ...

* /mnt/etc/network/interfaces.local: IP address and mask, and the local
  gateway address.

* /mnt/etc/resolv.conf: the usual; domain and DNS server(s).

* /mnt/etc/hostname; the hostname of course

* /mnt/etc/hosts: IP Hostname of course

* /etc/timezone: correct timezone for the target!
* /etc/localtime: Copy proper file from /usr/share/zoneinfo

* Set the root password; we do not want it the same on each control
  node, although note that ssh root login is not allowed. Be sure
  to write it down sompelace.

	sudo chroot /mnt passwd root

* Generate fresh ssh host heys:

     sudo chroot /mnt ssh-keygen -t rsa1 -b 1024 -f /etc/ssh/ssh_host_key -N ''
     sudo chroot /mnt ssh-keygen -t dsa -f /etc/ssh/ssh_host_dsa_key -N ''
     sudo chroot /mnt ssh-keygen -t ecdsa -f /etc/ssh/ssh_host_ecdsa_key -N ''

* Create the initial admin account for the whoever said they are the
  local admin. This requires an ssh version 2 pub key. Copy that file
  to /mnt/tmp, and then:

  Note: there is probably an old admin account in the passwd file,
  which needs to be deleted. Look at the end of /mnt/etc/passwd
  and if so, do this first:
  
	sudo chroot /mnt /usr/local/bin/mkadmin.pl -r YYYY
  then:
	sudo chroot /mnt /usr/local/bin/mkadmin.pl XXXX /tmp/key.pub

Now we want to take an imagezip of the mirror disk. This will give us
an ndz file that we can imageunzip onto the control node disk (this is
discussed in great detail in the installation notes in this directory).

	sudo umount /mnt/usr /mnt
	sudo imagezip -o /dev/sdb /scratch/newrack.ndz

Once the imagezip is done, you want to copy it over to Utah's www
downloads directory so that it easily available to the new control
node. Then remount the filesystems:

	sudo mount /mnt
	sudo mount /mnt/usr


