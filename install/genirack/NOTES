Setting up an InstaGeni Rack. First, we need the following info:

1. Hostname for the control node.
2. IP and netmask for the control node.
3. IP for the ilo interface on the control node (same netmask).
4. IP for your DNS server.
5. IP for your default router.
6. Login (email), and ssh version 2 public key for a local administrator.
   This should be a real person, not a pseudo user.
7. The iLo passwords for all of the nodes. These are stamped on the
   top of each of the nodes. Please note the rack slot number for each
   password. Ideally, we would have the ethernet addresses for the ilo
   and eth0 interfaces too, but I expect we will have to figure that
   out on the fly using the dhcpd logs.
8. External network information.
     A. We need to know what port on the data switch will bring in the
        openflow vlan (1750). If you have the all-copper module, we
        suggest that you use port 24 for the upwire to the switch
        providing that vlan. If you use a different port, be sure to
        tell us!
     B. We also need to know any other external connections such as
        ION, NLR, etc. For each of these connections, we need to know
        what port on the data plane switch it comes in on, what vlan
        numbers are available, and the endpoint. For example, here is
        the info for the BBN rack:
           * VLAN IDs 3747-3749 to Internet2
	     (ION endpoint bbn.newy.ion.internet2.edu)
           * VLAN IDs 2644-2649 to NLR
	     (FrameNet endpoint bost.layer2.nlr.net[Gi9/2])
     C. If you are really ambitious, you can generate a wiki page like
        this: http://groups.geni.net/geni/wiki/SiteBbn  :-)

* Send Utah all of the above info so that we can "bake" the images
  for you. Once you hear back from us, you may continue with these
  instructions.

  Waiting, waiting, waiting ...

* Power on the control switch (Procurve 2610 in the top slot).

* Attach a console to the control node and power on the node. You will
  wait a while for the "HP ProLiant" screen. Wait until it says "press
  any key for Option ROM messages" and then right after the screen
  switches, type F8 to get into the iLo configuration. Gotta be fast
  on this. If you miss it, power cycle. 

* Once the iLo screen comes up, right arrow to Network and choose the
  DHCP option. You want to make sure DHCP is off. F10 to save and then
  esc to go back. Then choose the NIC option. Fill in the iLo
  IP/Mask/Router, then F10 to save and then esc to exit. iLo will
  reset.

* Is the RAID array setup? It wasn't on Utah's rack. Need to fill this
  part in. 

* At this point, Utah can do the rest of the rack setup without
  further intervention from you. Well, unless something gets wedged
  and Utah needs something to be physically power cycled. Or you can
  go back to your desk and proceed to setup the rack using the
  following instructions. Which do you prefer? I know you will make
  the correct choice.

* Now that you have decided, send email to Utah asking us to complete
  the installation while you go back to working on other projects. 

* Using your web browser, go to the iLo IP you set, and login using
  Administrator and the iLo password that is stamped on top of the
  control node, or in the data file you received.

* Click on the Remote Console tab, and then on remote console. Click
  on the java launch button. Wait for the popup boxes that ask you
  to trust it. Say yes of course, and wait for the console window to
  appear.

* Go to the Virtual Media tab, and then on the right hand side specify
  the url of the boot ISO image. This will be something like:
  
	http://155.98.32.70/downloads/genirack.iso
	
  which is Utah's web server. Check the box to boot from the CD on the
  next reset. The click on "Insert Media", and then after you get the
  confirmation that it attached okay, click the reset button.
  
* Wait for the node to boot. It should boot from the virtual CD drive
  since there is no other boot media, but if not you can hit F11 on
  the next go around, which will give you a list of options. Type
  whatever number is to the left of the CD choice. 

* The ISO will load and give you a boot prompt. Just hit return. You
  will eventually get a shell prompt after a lot of noisy output.

* Fire up the network:

	ifconfig eth0 inet Control_IP netmask Control_Mask
	route add default gw Gateway_IP

  The Control_IP is *NOT* the iLo IP you used above. It is the IP you
  have assigned to the control node itself.

* Transfer the control node image from Utah:

	cd /tmp
	wget http://155.98.32.70/downloads/genirack-1.ndz

  This is about a 1GB so it will take a while.

* Write the image file to the disk using the Emulab decompression tool:

	/usr/bin/imageunzip -o genirack-1.ndz /dev/cciss/c0d0

  This will take a little while. Or a long while. 

* Set the boot order so that the control node does not try to boot
  from the network, unless all else fails.

	cd /TOOLKIT
	./setbootorder floppy cdrom usb hd pxe

* Type "reboot" at the shell prompt. With any luck, the node will boot
  first time.

* Make sure all five of the experimental nodes are fully powered off;
  the ilo has to be off, and the easiest thing to do is just unplug
  them. 

* At this point we need to configure the switches with static IP
  addresses. This is a bit tricky since the only way to do this is to
  have DHCP return a dynamic address so we can telnet to the switch
  long enough to change its address. Yes, this is a pain but we cannot
  have multiple DHCP servers running since that would confuse boss.
  Login to the control node as root.

	sudo /usr/sbin/dhcpd -4 -cf /etc/dhcp/dhcpd.conf xenbr1 xenbr3

  Now power on the experimental switch, and power cycle the control
  switch so that it will restart its dhcp cycle. You will lose
  connectivity to the control node for a moment or two.

  Once you get connectivity back, tail the end of the /var/log/syslog to
  see if the switches have made their DHCP requests.

* At some point you will be able to telnet to 10.1.1.253 and 10.3.1.253.
  These are the IPs assigned by dhcp.

  First telnet to 10.1.1.253. This is the control switch (2610). You want
  to issue the following commands at the prompt.

  2610> config
  2610(config)> vlan 11
  2610(vlan-11)> name control-alternate
  2610(vlan-11)> untagged 24        XXXX Make sure about port number!
  2610(vlan-11)> ip address 10.2.1.253/24
  2610(vlan-11)> exit
  2610(config)> write memory

  Now logout and telnet to 10.3.1.253. This is the alternate link
  to the procurve that allows us to configure without a serial link,
  and hopefully maintain a connection in case something goes wrong.
  More fundimentally, once we remove the ip address from the default
  vlan, we would no longer be able to get back in on that IP until
  we can recreate it on the private vlan. Basically, we are doing a
  switcherroo.
  
  2610> config
  2610(config)> vlan 10
  2610(vlan-10)> name control-hardware
  2610(vlan-10)> untagged 23	   XXXX Make sure about port number!
  2610(vlan-10)> ip address 10.1.1.253/24
  2610(vlan-10)> exit
  2610(config)> vlan 1
  2610(vlan-1)> ip address 10.254.254.253/24  # IGMP querier requires this
  2610(vlan-1)> exit
  2610(config)> management-vlan 10
  2610(config)> ip default-gateway 10.1.1.254
  2610(config)> vlan 1 ip igmp
  2610(config)> vlan 1 ip igmp querier
  2610(config)> no web-management
  2610(config)> no snmp-server community public
  2610(config)> snmp-server community XXXXX manager unrestricted
  2610(config)> password all (type in same password for manager/operator)
  2610(config)> write memory
  2610(config)> reload

  REMEMBER YOUR PASSWORD and COMMUNITY XXXXX
  REMEMBER YOUR PASSWORD and COMMUNITY XXXXX
  REMEMBER YOUR PASSWORD and COMMUNITY XXXXX

  The switch will take moment to reset so you might lose your connection
  to the control node.

* For the experimental switch, we need to do something like above,
  which is move the ip address from the default vlan to a private
  vlan, but in this case we can do it from the serial console, and
  so it is a lot easier. We use minicom:

	sudo screen /dev/ttyS0 115200

  wait for it to sync up and then you will get the prompt.

  5400> config
  5400(config)> vlan 10
  5400(vlan-10)> name control-hardware
  5400(vlan-10)> untagged E20	  XXXX Make sure about port number!
  5400(vlan-10)> ip address 10.3.1.253/24
  5400(vlan-10)> exit
  5400(config)> no vlan 1 ip address
  5400(config)> management-vlan 10
  5400(config)> ip default-gateway 10.3.1.254
  5400(config)> no web-management
  5400(config)> no snmp-server community public
  5400(config)> snmp-server community XXXXX manager unrestricted
  5400(config)> password all (type in same password for manager/operator)
  5400(config)> write memory
  5400(config)> reload
  
  USE THE SAME PASSWORD and COMMUNITY XXXXX
  USE THE SAME PASSWORD and COMMUNITY XXXXX
  USE THE SAME PASSWORD and COMMUNITY XXXXX

  Some 6600 specific stuff. The port number above is 48.

  6600> config
  6600(config)> oobm disable
  6600(config)> snmp-server listen data
  6600(config)> write memory
  
* The 5400 will take longer to reset. Be sure you can telnet back to the
  both switches and login with the password you used above. Once you are
  sure that is the case, kill off dhcpd on the control node.

	sudo killall dhcpd

  Make sure it died by doing:

	ps auxwww | grep dhcpd

* Create the 4th partition in the partition table:

	fdisk /dev/sda

  Use the "n" option, primary partition type, default start, +1000G, "w"
  to write it out.

  Then inform the kernel:

	partprobe -s

* Initialize the LVM partition. We use LVMs for the boss/ops filesystems.

	pvcreate /dev/sda4
	vgcreate xen-vg /dev/sda4
    	vgchange -a y xen-vg

* Create a filesystem to hold the boss/ops tarballs. These are pretty
  big but will be deleted after we copy the filesystems into their own
  lvms.

	mkdir /scratch
	/sbin/lvcreate -n scratch -L 75G xen-vg
	mke2fs -j /dev/xen-vg/scratch
	mount /dev/xen-vg/scratch /scratch
	chmod 777 /scratch

* Copy boss.tar.gz and ops.tar.gz to /scratch on the control node, and
  then unpack them. There will be two directories, ops and boss.

* Restore the VMs:

        mkdir ~elabman/boss ~elabman/ops
	~elabman/restorevm.pl -t ~elabman/boss -i boss /scratch/boss
	~elabman/restorevm.pl -t ~elabman/ops  -i ops /scratch/ops

  This creates a bunch of LVMs and rewrites the xm.conf in the
  boss/ops directories to reflect the new LVM paths, etc.

* Fire up the VMs. Ops has to be first, followed by boss.

	sudo xm create ~elabman/ops/xm.conf
	sleep 30
	sudo xm create ~elabman/boss/xm.conf

* It is possible that ops will hang on fixarp, because tmcd is not
  running on boss yet. Log into boss and do:

	sudo testbed-control boot

  which should get ops running.

* Now it is time to power on the experimental nodes. If all goes well,
  they will boot up into FreeBSD MFS and be in the hwdown experiment.
  Before we release them, we want to change some settings on the ilo.
  The following command will change the Admin password, create an
  elabman user, load its ssh key, change the boot order, etc, etc.
  This part is not scripted yet, so need to look at the XML file
  /usr/testbed/etc/genirack/ilo.xml to get the ilo passwords. 

  In this file, the mapping of pcX to slot is:

	pc1 -> U05
	pc2 -> U06
	pc3 -> U07
	pc4 -> U08
	pc5 -> U09

  So grab the ilo password for each one, and then:

	perl /usr/testbed/obj/testbed/install/genirack/initilo.pl <ilopswd> pc1
	perl /usr/testbed/obj/testbed/install/genirack/initilo.pl <ilopswd> pc2
	perl /usr/testbed/obj/testbed/install/genirack/initilo.pl <ilopswd> pc3
	perl /usr/testbed/obj/testbed/install/genirack/initilo.pl <ilopswd> pc4
	perl /usr/testbed/obj/testbed/install/genirack/initilo.pl <ilopswd> pc5
	perl /usr/testbed/obj/testbed/install/genirack/initilo.pl <ip> <ilopswd>

  The last line updates the control node ilo.

* Free all the nodes up and lets hope they reload okay. Okay, lets
  just do one to start with.

	wap nfree emulab-ops hwdown pc1

  If that works and pc1 does indeed go into the free pool, then do the
  rest of the nodes:

  	wap nfree emulab-ops hwdown pc2 pc3 pc4 pc5

* Enable this site in Utah:

	sudo cacontrol -c boss.XXX.XXX.XXX

* On the new boss, need to reload the bundles:

	sudo /usr/testbed/sbin/protogeni/getcacerts

* Arrange for the VMs to auto starts:

	cd /etc/xen/auto/
	sudo ln -s ~stoller/ops/xm.conf 1.ops.conf
	sudo ln -s ~stoller/boss/xm.conf 2.boss.conf

* Next we want to update the firmware on the data plane switch to the
  one that supports openflow. First the firmware from Utah to the
  local tftp directory on boss.

	cd /tftpboot
	sudo wget http://www.emulab.net/downloads/K_15_06_5008.swi

* Log into procurve2 using the password in /usr/testbed/etc/switch.pswd
  We then want to make a copy of the current config in case we have to
  revert back.

	5406> show config files
	5406> copy config config1 config config-save
       
* Now load the openflow firmware into the primary flash. First make
  sure the secondary has a copy of the primary. 

	5406> show flash
	5406> copy tftp flash 10.3.1.1 K_15_06_5008.swi

* And if that works, reboot the switch.

	5406> reload

* Create some test experiments.

---
Problems:

root/toor password on boss did not match what is in the file.
root/toor password on ops is generated elabinelab swapin and needs to set.
ntpd and ntpdate failures.

SSH enable:

ip ssh public-key manager "ssh-rsa AAA ..."
# aaa authentication ssh enable public-key
HP-E2620-24(config)# aaa authentication ssh login public-key 

