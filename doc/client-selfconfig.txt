I. Introduction.

Emulab uses a largely "node driven" strategy for node configuration; i.e.,
it is a pull-style process with the nodes requesting information from Emulab
Central and configuring themselves.

We use the same basic framework for configuring:
 * local cluster physical and virtual nodes
 * local wireless nodes
 * IXP network processors
 * Stargate SBCs
 * remote internet nodes
 * planetlab "nodes" (slivers)
and for OSes including:
 * FreeBSD 4, 5, 6, and 7
 * OpenBSD 3
 * Redhat 7, 9 and Fedora 4, 6
 * Ubuntu 7.0
 * Windows XP

The self-configuration strategy requires that the OSes to be run on nodes
have Emulab client code installed to handle the self-configuration.  The
configuration code is mostly perl and shell scripts, with a couple of
C and C++ programs.  The code resides mostly in the /etc/emulab and
/usr/local/etc/emulab directories on the node with a single hook placed
in /etc/rc.d (or /etc/rc.local or ...) to trigger the configuration process.

In the case of virtual nodes (or "subnodes" which are otherwise dependent on
a "host"; e.g., IXPs), the configuration may be split between the physical
(host) and virtual (sub) nodes.  In this situation, the physical node boots
and begins self-configuration via the scripts.  Part of that self-config in
this case is to create, initialize and boot any virtual nodes that it hosts.
Typically, the physical host will perform only the initializations that the
virtual hosts themselves do not have the privilege to do; e.g., initializing
(virtual) network interfaces.  Ideally, most virtual node configuration will
then be done in the context of the vnodes themselves.  This requires that the
virtual nodes' filesystems contain a copy of the Emulab startup scripts.

The first step in the configuration process if for a node to identify the
so-called "boss" node (Emulab Central) and (possibly) configure the network
to talk to it.  How this is done depends on the type of the node.  For
cluster and wireless physical nodes which are connected directly to the
Emulab control network, the nodes DHCP to discover and configure the control
network and then (somewhat arbitrarily) use the returned name server as the
boss node.  Remote physical nodes have the boss identity hardwired in (usually
on a CD, flash dongle, or floppy) and use their default IP routing setup to
reach it.

As mentioned there are a set of scripts, each responsible for initializing
some part of a node's configuration; e.g., accounts, NFS shared mount points,
network interfaces, routing, etc.  The scripts are invoked with one of four
actions: boot, shutdown, reconfig or reset.  The first three pretty much do
what you would expect: set things up at boot time, tear things down at
shutdown, and perform a dynamic reconfiguration while running.  The last is
a special action used when creating disk images; it is invoked to cleanup
any on-disk state created by the boot or reconfig actions.

While the same core set of scripts and libraries are used for initializing
all node types, many of the scripts have large quantities of conditionalized
code for various node types.  In the OS dimension, there is much less runtime
conditionalized code.  There is a common set of scripts and libraries for
all OSes and distinct sets of per-OS scripts and libraries.

In general, the scripts operate in one of two ways: either they do the
appropriate setup themselves, or they create another set of shell scripts
in /var/emulab/boot which are invoked to do the appropriate setup.  The
exact intent of the latter method is lost in the mists of time, but I think
we had envisioned a more efficient boot path, with nodes contacting Emulab
Central (see the next paragraph) only at initial boot up and then creating
the "fast path" boot scripts to use for future reboots.  However, this is
not what we do now.  Reboots and reconfigurations always re-contact Emulab
Central.

Most of the configuration information is passed to the nodes through the
Testbed Master Control Protocol via the programs tmcc (the client) and
tmcd (the server).  Note that we generally refer to the protocol as TMCC
or TMCD rather than TMCP, because...well, just because that is what we do!

TMCC/TMCD/TMCP is a custom ASCII-based, total hack protocol.  The API of
TMCC is eclectic.  For the most part it consists of single word requests
about a particular type of information for a single node (e.g., "ifconfig",
"routes", "accounts").  The returned information is in one or more
easily-parsable lines of KEY=VALUE pairs.  Typically, the target node
of the command is implied by the IP address from which the request comes,
though the target may also be explicitly specified.

Evolution of the API is largely "convenience driven."  If there is something
small we need quickly, we tend to add a command to TMCC in the most
straightforward manner.  TMCC is by no means the future of node-configuration
protocols.

The tmcc client is just a mechanism for returning DB information, it does
not perform any actions on its own.  For example you use tmcc to request
information about which user accounts should be set up on a node, it does
not actually modify the password file.  Invocations of tmcc are made from
shell/perl scripts which do the actual client-side customization.  The
client can communicate with the server via UDP, TCP or SSL over TCP.
Some commands can only be performed over certain transports.  For example,
account info can only be returned via SSL+TCP since a password hash is
part of the returned info.

Security in TMCC.  You might want to sit down for this.  Authentication of
node TMCC requests to Emulab Central is provided in a couple of ways.

II. Rough order of RC files called [ and tmcc calls used ].

 * identify boss [ bossinfo ]
 * rc.{linux,freebsd}
 * rc.kname
 * run tbshutdown (a daemon)

 Node enters the TBSETUP state

 * bootsetup [ status ]
 * get all tmcc info [ fullconfig ]
 * find node role [ role ]
 * rc.ipod [ ipodinfo ]
 * rc.healthd
 * rc.slothd
 * watchdog [ watchdoginfo isalive ntpdrift rusage hostkeys ]
 * rc.config
   * rc.firewall [ firewallinfo ]
   * rc.misc [ nodeid creator ]
   * rc.localize [ localization ]
   * rc.keys [ keyhash eventkey ]
   * rc.mounts [ mounts ]
   * rc.topomap [ topomap (if not over NFS) ]
   * rc.accounts [ accounts ]
   * rc.route [ routing ]
   * rc.tunnels [ tunnel ]
   * rc.ifconfig [ ifconfig ]
   * rc.delays [ delay linkdelay ]
   * rc.hostnames [ hosts (if not over NFS) ]
   * rc.trace [ traceinfo ]
   * rc.syncserver [ syncserver ]
   * rc.trafgen [ traffic ]
   * rc.tarfiles [ tarball ]
   * rc.rpms [ rpm ]
   * rc.progagent [ programs userenv plabconfig ]
   * rc.linkagent [ ifconfig ]
 * rc.canaryd
 * rc.linktest

 Enter ISUP state

 * rc.startcmd [ startup ]
 * bootvnodes [ vnodelist ]

III. TMCC calls:

bossinfo:
    Return info about the "boss" server to which to make TMCC calls.

    <bossnode name> <bossnode IP>

    Obviously, info isn't obtained from boss!  Either compiled in or
    uses current nameserver or contents of a local file.

status:
    Return pid/eid of containing experiment (or "free") and user-specified
    name of the node.

    ALLOCATED=<pid>/<eid> NICKNAME=<vname>

fullconfig:
    Single call to return the bulk of node-specific TMCC info.  Optimization
    to avoid numerous calls to boss at node boot time.

    <tons of stuff>

role:
    Return role of node inside Emulab experiment, usually one of:
    node, virthost, delaynode.

    <role>

ipodinfo:
    Return info for configuring the ICMP "ping of death" in the kernel:
    hostIP, netmask and optional hash code (newly generated on every call).

    HOST=<boss IP> MASK=<boss netmask> HASH=<32-char hash>

topomap:
    Returns (binary) contents of compressed topology map file.  One of
    the few non-ascii returns.

    <binary data>

accounts:
    Returns info about users and groups to setup on the node, one line per
    user or group.

    ADDGROUP NAME=<UNIX groupname> GID=<numeric UNIX gid>

    ADDUSER LOGIN=<UNIX username> PSWD=<password hash> UID=<UNIX uid>
      GID=<UNIX gid> ROOT=<1 or 0> NAME=<user full name>
      HOMEDIR=<homedir path> GLIST=<list of additional UNIX gids>
      SERIAL=<unique serial #> EMAIL=<email address> SHELL=<login shell>

routing:
   Return the "type" of IPv4 routing used and possibly a list of route
   descriptions.  Type is one of: none, ospf, static, manual.  If type
   is manual, a list of routes is also returned.  If type is ospf, a
   router daemon is to be run on the node.  If type is static, static
   route calculation is done on the node.

   ROUTERTYPE=<type>

   ROUTE DEST=<host or net IP> DESTTYPE=<host|net> DESTMASK=<netmask>
      NEXTHOP=<IP> COST=<metric>

tunnel:
   Return information about over-IP tunnels to setup.  This command was
   used for wide-area nodes and has likely bitrotted.

   TUNNEL=<tunneldev?> ISSERVER=<1|0> PEERIP=<IP> PEERPORT=<port #>
      PASSWORD=<password> ENCRYPT=<??> COMPRESS=<1|0>
      INET=<IP> MASK=<netmask> PROTO=<tunnel proto>

ifconfig:
    Return info about IPv4 interfaces to configure, one line per interface.
    Used for both standard interfaces and "virtual" interfaces.  Interfaces
    are identified on nodes by their MAC addresses as they have different
    "names" under Linux, BSD or Windows and may even move around on the
    same OS.  Some interface types (currently just wireless interfaces)
    may have additional info specified via INTERFACE_SETTING lines, one
    line per key/value pair on each such interface.  Virtual interfaces
    have different info returned.  Such interfaces can be type: veth
    (Utah-local virtual ethernet interface), vlan (802.1q tagged vlan),
    or alias (an IP alias on an existing interface).

    INTERFACE IFACETYPE=<typename> INET=<IP> MASK=<netmask> MAC=<MACaddr>
      SPEED=<100Mbps|1000Mbps> DUPLEX=<half|full> IFACE=<|ixp>
      RTABID=<route table #> LAN=<link/lan vname>

    INTERFACE IFACETYPE=<veth|vlan|alias> INET=<IP> MASK=<netmask>
      ID=<iface ID> VMAC=<virtual MAC> PMAC=<associated real MAC>
      RTABID=<route table #> ENCAPSULATE=<1|0> LAN=<link/lan vname>
      VTAG=<802.1q VLAN tag>

    INTERFACE_SETTING MAC=<MACaddr> KEY=<attrkey> VAL=<attrval>
 
hostnames:
    Return host name info for all other hosts in the experiment, one line
    per host.  Info includes a canonical name, aliases, and the IP.  The
    canonical name is formed by concatenating the vname of the target host
    and the vname of the link it is reached over.

    NAME=<canon name> IP=<IP> ALIASES=<list of alias names>

    Used to generate the local /etc/hosts file.  Note that not all of these
    nodes are necessarily reachable, it depends on whether the topo is
    fully connected and whether routing has been setup.

tarball:
    Return a list of tarballs to download and install on the node,
    one line per tarball.  The returned tarball path is an absolute path
    on the server.

    DIR=<dir to "cd" to before install> TARBALL=<tarfile>

    The tarball itself can be fetched either across NFS (if on a node-shared
    filesystem) of via HTTPS from the Emulab boss (see the os/install-tarfile
    script).

rpm:

    Return a list of RPMs to download and install on the node, one line
    per RPM.  The returned RPM path is an absolute path on the server.

    RPM=<RPM filename>

    The RPM itself can be fetched either across NFS (if on a node-shared
    filesystem) of via HTTPS from the Emulab boss (see the os/install-rpm
    script).
